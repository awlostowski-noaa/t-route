{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "# logic to accomodate Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    ENV_IS_CL = True\n",
    "\n",
    "    root = pathlib.Path(\"/content/t-route\").resolve()\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"https://github.com/NOAA-OWP/t-route.git\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ! pip install geopandas\n",
    "    ! pip install netcdf4\n",
    "\n",
    "except:\n",
    "    ENV_IS_CL = False\n",
    "    root = pathlib.Path(\"..\").resolve()\n",
    "\n",
    "routing_v02_dir = os.path.join(root, \"src\", \"python_routing_v02\",\"fast_reach\")\n",
    "sys.path.append(routing_v02_dir)\n",
    "\n",
    "framework_v02_dir = os.path.join(root, \"src\", \"python_framework_v02\")\n",
    "sys.path.append(framework_v02_dir)\n",
    "\n",
    "framework_v01_dir = os.path.join(root, \"src\", \"python_framework_v01\")\n",
    "sys.path.append(framework_v01_dir)\n",
    "\n",
    "# scientific packages\n",
    "import time\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from itertools import chain, islice\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "# t-route functions\n",
    "import nhd_network_utilities_v02 as nnu\n",
    "import nhd_io\n",
    "import nhd_network\n",
    "import network_dl\n",
    "import mc_reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the routing model timestep\n",
    "#----------------------------------------------------------------------#\n",
    "dt = 5*60  # routing simulation timestep (seconds) \n",
    "nts = 100\n",
    "\n",
    "# specify supernetwork connections set\n",
    "#----------------------------------------------------------------------#\n",
    "test_folder = os.path.join(root, r\"test\")\n",
    "geo_input_folder = os.path.join(test_folder, r\"input\", r\"geo\")\n",
    "supernetwork = \"CapeFear_FULL_RES\"\n",
    "\n",
    "network_data = nnu.set_supernetwork_data(\n",
    "    supernetwork=supernetwork, geo_input_folder=geo_input_folder\n",
    ")\n",
    "\n",
    "# if the NHDPlus RouteLink file does not exist, download it.\n",
    "if not os.path.exists(network_data[\"geo_file_path\"]):\n",
    "    filename = os.path.basename(network_data[\"geo_file_path\"])\n",
    "    network_dl.download(network_data[\"geo_file_path\"], network_data[\"data_link\"])\n",
    "\n",
    "# select only the necessary columns of geospatial data, set the DataFrame index\n",
    "cols = [v for c, v in network_data[\"columns\"].items()]\n",
    "data = nhd_io.read(network_data[\"geo_file_path\"])\n",
    "data = data[cols]\n",
    "data = data.set_index(network_data[\"columns\"][\"key\"]).astype(\"int\")\n",
    "\n",
    "# mask NHDNetwork to isolate test network of choice\n",
    "if \"mask_file_path\" in network_data:\n",
    "    data_mask = nhd_io.read_mask(\n",
    "        network_data[\"mask_file_path\"], layer_string=network_data[\"mask_layer_string\"],\n",
    "    )\n",
    "    data = data.filter(data_mask.iloc[:, network_data[\"mask_key\"]], axis=0)\n",
    "\n",
    "# sort index\n",
    "data = data.sort_index()\n",
    "\n",
    "# replace downstreams\n",
    "data = nhd_io.replace_downstreams(data, network_data[\"columns\"][\"downstream\"], 0)\n",
    "\n",
    "# add a dt column to the data DataFrame\n",
    "data[\"dt\"] = dt\n",
    "\n",
    "# rename columns to specific variable names expected by mc_reach.compute_network\n",
    "column_rename = {\n",
    "    network_data[\"columns\"][\"dx\"]: \"dx\",\n",
    "    network_data[\"columns\"][\"tw\"]: \"tw\",\n",
    "    network_data[\"columns\"][\"twcc\"]: \"twcc\",\n",
    "    network_data[\"columns\"][\"bw\"]: \"bw\",\n",
    "    network_data[\"columns\"][\"ncc\"]: \"ncc\",\n",
    "    network_data[\"columns\"][\"s0\"]: \"s0\",\n",
    "    network_data[\"columns\"][\"cs\"]: \"cs\",\n",
    "    network_data[\"columns\"][\"n\"]: \"n\",\n",
    "}\n",
    "data = data.rename(columns=column_rename)\n",
    "\n",
    "\n",
    "# extract downstream connections for each node\n",
    "connections = nhd_network.extract_connections(data, network_data[\"columns\"][\"downstream\"])\n",
    "\n",
    "# organize network into reaches\n",
    "#----------------------------------------------------------------------#\n",
    "\n",
    "# reverse the network - track upstream connections\n",
    "rconn = nhd_network.reverse_network(connections)\n",
    "\n",
    "# isolate independent subnetworks\n",
    "subnets = nhd_network.reachable_network(rconn)\n",
    "\n",
    "# identify the segments in each subnetwork\n",
    "subreachable = nhd_network.reachable(rconn)\n",
    "\n",
    "# break each subnetwork into reaches\n",
    "subreaches = {}\n",
    "for tw, net in subnets.items():\n",
    "    path_func = partial(nhd_network.split_at_junction, net)\n",
    "    subreaches[tw] = nhd_network.dfs_decomposition(net, path_func)\n",
    "\n",
    "# Load Lateral Inflows\n",
    "#----------------------------------------------------------------------#\n",
    "# path = os.path.join(root, \"test\",\"input\",\"geo\",\"NWM_2.1_Sample_Datasets\",\"Pocono_TEST1\",\"example_CHRTOUT\")\n",
    "# qlat_files = glob.glob(path + \"/*\", recursive=True)\n",
    "# ql = nhd_io.get_ql_from_wrf_hydro(qlat_files,index_col=\"feature_id\")\n",
    "\n",
    "def step_qlats(data, nsteps, qlat):\n",
    "\n",
    "    q1 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q2 = np.full((len(data.index), nsteps // 10), qlat, dtype=\"float32\")\n",
    "    q3 = np.full((len(data.index), nsteps // 10), qlat, dtype=\"float32\")\n",
    "    q4 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q5 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q6 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q7 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q8 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q9 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q10 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "\n",
    "    q = np.concatenate((q1, q2, q3, q4, q5, q6, q7, q8, q9, q10), axis=1)\n",
    "\n",
    "    ql = pd.DataFrame(q, index=data.index, columns=range(nsteps))\n",
    "\n",
    "    return ql\n",
    "\n",
    "# create the lateral inflow data\n",
    "ql = step_qlats(data, nts, 10.0)\n",
    "\n",
    "# Load Initial Conditions\n",
    "#----------------------------------------------------------------------#\n",
    "# wrf_hydro_channel_restart_file = os.path.join(root, \"test\", \"input\",\"geo\",\"NWM_2.1_Sample_Datasets\",\"Pocono_TEST1\",\"example_RESTART\",\n",
    "#                     \"HYDRO_RST.2017-12-31_06-00_DOMAIN1\")\n",
    "# wrf_hydro_channel_ID_crosswalk_file = os.path.join(root, \"test\", \"input\",\"geo\",\"NWM_2.1_Sample_Datasets\",\"Pocono_TEST1\",\"primary_domain\",\n",
    "#                     \"DOMAIN\",\"Route_Link.nc\")\n",
    "# wrf_hydro_channel_ID_crosswalk_file_field_name = \"link\"\n",
    "\n",
    "# q0 = nhd_io.get_stream_restart_from_wrf_hydro(\n",
    "#     wrf_hydro_channel_restart_file,\n",
    "#     wrf_hydro_channel_ID_crosswalk_file,\n",
    "#     wrf_hydro_channel_ID_crosswalk_file_field_name\n",
    "# )\n",
    "q0 = pd.DataFrame(10,index = data.index, columns = [\"qu0\",\"qd0\",\"h0\"], dtype = \"float32\")\n",
    "\n",
    "# Build Time Domain\n",
    "#----------------------------------------------------------------------#\n",
    "# dt_routing = pd.Timedelta(str(dt) + 'seconds')\n",
    "# wrf_time = ql.columns.astype(\"datetime64[ns]\")\n",
    "# dt_wrf = (wrf_time[1] - wrf_time[0])\n",
    "# sim_duration = (wrf_time[-1] + dt_wrf) - wrf_time[0]\n",
    "# nts = round(sim_duration / dt_routing)\n",
    "\n",
    "# Create order-based groupings\n",
    "#----------------------------------------------------------------------#\n",
    "tuple_reaches = {}\n",
    "for tw, net in subnets.items():\n",
    "    path_func = partial(nhd_network.split_at_junction, net)\n",
    "    tuple_reaches[tw] = nhd_network.dfs_decomposition_depth_tuple(net, path_func)\n",
    "\n",
    "# convert tuple_reaches (dict) to a (list)\n",
    "overall_tuple_reaches = []\n",
    "for keys, tuple_list in tuple_reaches.items():\n",
    "    overall_tuple_reaches.extend(tuple_list)\n",
    "\n",
    "# create a dict with key = reach order, and values are segments in each reach of the corresponding order\n",
    "overall_ordered_reaches_dict = nhd_network.tuple_with_orders_into_dict(overall_tuple_reaches)\n",
    "max_order = max(overall_ordered_reaches_dict.keys())\n",
    "\n",
    "overall_ordered_reaches_list = []\n",
    "ordered_reach_count = []\n",
    "ordered_reach_cache_count = []\n",
    "for o in range(max_order,-1,-1):\n",
    "    overall_ordered_reaches_list.extend(overall_ordered_reaches_dict[o])\n",
    "    ordered_reach_count.append(len(overall_ordered_reaches_dict[o]))\n",
    "    ordered_reach_cache_count.append(sum(len(r) for r in overall_ordered_reaches_dict[o]))\n",
    "    \n",
    "rconn_ordered = {}\n",
    "rconn_ordered_byreach = {}\n",
    "for o in range(max(overall_ordered_reaches_dict.keys()),-1,-1):\n",
    "    rconn_ordered[o] = {}\n",
    "    for reach in overall_ordered_reaches_dict[o]:\n",
    "        for segment in reach:\n",
    "            rconn_ordered[o][segment] = rconn[segment]\n",
    "            rconn_ordered_byreach[segment] = rconn[segment]\n",
    "            \n",
    "# change variables to type float32, as expected by mc_reach.compute_network\n",
    "data = data.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.401489734649658\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()    \n",
    "for i in range(max(overall_ordered_reaches_dict.keys()),-1,-1):\n",
    "    \n",
    "        for r in overall_ordered_reaches_dict[i]:\n",
    "            \n",
    "            data_sub = data.loc[\n",
    "                r, [\"dt\", \"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\"]\n",
    "            ].sort_index()\n",
    "            qlat_sub = ql.loc[r].sort_index()\n",
    "            q0_sub = q0.loc[r].sort_index()\n",
    "            mc_reach.compute_network(\n",
    "                    nts,\n",
    "                    [r],\n",
    "                    {key: [] for key in r}, # this needs to be the reverse connections dictionary, keys are the segments in r \n",
    "                    data_sub.index.values.astype(\"long\"),\n",
    "                    data_sub.columns.values,\n",
    "                    data_sub.values,\n",
    "                    qlat_sub.values,\n",
    "                    q0_sub.values,\n",
    "                )\n",
    "\n",
    "\n",
    "tf = time.time()\n",
    "print(tf - t0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.61048102378845\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()    \n",
    "for i in range(max(overall_ordered_reaches_dict.keys()),-1,-1):\n",
    "    \n",
    "    with Parallel(n_jobs=-1, backend=\"threading\") as parallel:\n",
    "        jobs = []\n",
    "        for r in overall_ordered_reaches_dict[i]:\n",
    "            \n",
    "            data_sub = data.loc[\n",
    "                r, [\"dt\", \"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\"]\n",
    "            ].sort_index()\n",
    "            qlat_sub = ql.loc[r].sort_index()\n",
    "            q0_sub = q0.loc[r].sort_index()\n",
    "            jobs.append(\n",
    "                delayed(mc_reach.compute_network)(\n",
    "                    nts,\n",
    "                    [r],\n",
    "                    {key: [] for key in r}, # this needs to be the reverse connections dictionary, keys are the segments in r \n",
    "                    data_sub.index.values.astype(\"long\"),\n",
    "                    data_sub.columns.values,\n",
    "                    data_sub.values,\n",
    "                    qlat_sub.values,\n",
    "                    q0_sub.values,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        results = parallel(jobs)\n",
    "\n",
    "tf = time.time()\n",
    "print(tf - t0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8971974849700928\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "t0 = time.time()   \n",
    "for twi, (tw, reach) in enumerate(subreaches.items(), 1):\n",
    "    r = list(chain.from_iterable(reach))\n",
    "    data_sub = data.loc[\n",
    "        r, [\"dt\", \"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\"]\n",
    "    ].sort_index()\n",
    "    qlat_sub = ql.loc[r].sort_index()\n",
    "    q0_sub = q0.loc[r].sort_index()\n",
    "    results.append(\n",
    "        mc_reach.compute_network(\n",
    "            nts,\n",
    "            reach,\n",
    "            subnets[tw],\n",
    "            data_sub.index.values,\n",
    "            data_sub.columns.values,\n",
    "            data_sub.values,\n",
    "            qlat_sub.values,\n",
    "            q0_sub.values,\n",
    "        )\n",
    "    )\n",
    "tf = time.time()\n",
    "print(tf - t0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reach_timeloop(nts, reach, rconn, flowveldepth, ql, data, q0, assume_short_ts):\n",
    "    \n",
    "    # loop through timesteps\n",
    "    timestep = 0\n",
    "    while timestep < nts:\n",
    "\n",
    "        #--------------------------------------------------#\n",
    "        # extract upstream boundary conditions: qup and quc \n",
    "        #--------------------------------------------------#\n",
    "        quc = 0.0\n",
    "        qup = 0.0\n",
    "        for i in rconn[reach[0]]:\n",
    "            quc += flowveldepth.loc[i,timestep].q\n",
    "            if timestep > 0:\n",
    "                qup += flowveldepth.loc[i,(timestep-1)].q\n",
    "            else:\n",
    "                qup += q0.loc[i].qd0\n",
    "\n",
    "        if assume_short_ts:\n",
    "                quc = qup\n",
    "\n",
    "        boundary = np.array([quc,qup], dtype = \"float32\")\n",
    "\n",
    "        #--------------------------------------------------#\n",
    "        # initialize parameter input array\n",
    "        #--------------------------------------------------#\n",
    "        parameter_inputs = np.zeros((len(reach), 10), dtype = \"float32\")\n",
    "\n",
    "        #--------------------------------------------------#\n",
    "        # fill parameter input array with lateral inflow data\n",
    "        #--------------------------------------------------#\n",
    "        parameter_inputs[:,0] = ql.loc[reach].iloc[:,(int(timestep/(nts/ql.shape[1])))]\n",
    "\n",
    "        #--------------------------------------------------#\n",
    "        # fill parameter input array with MC parameter data\n",
    "        #--------------------------------------------------#\n",
    "        parameter_inputs[:,1:10] = data[[\"dt\",\"dx\",\"bw\",\"tw\",\"twcc\",\"n\",\"ncc\",\"cs\",\"s0\"]].loc[reach].to_numpy()\n",
    "\n",
    "        #--------------------------------------------------#\n",
    "        # fill buffer with initial conditions: qdp and velp \n",
    "        #--------------------------------------------------#\n",
    "        previous_state = np.zeros((len(reach), 3), dtype = \"float32\")\n",
    "        if timestep > 0:\n",
    "            previous_state[:,0:3] = flowveldepth.loc[reach,(timestep-1)].to_numpy()\n",
    "        else:\n",
    "            previous_state[:,0] = q0.loc[reach].qd0\n",
    "            previous_state[:,2] = q0.loc[reach].h0\n",
    "\n",
    "        #--------------------------------------------------#\n",
    "        # initalize output buffer\n",
    "        #--------------------------------------------------#\n",
    "        output_buffer = np.zeros((len(reach), 3), dtype = \"float32\")\n",
    "\n",
    "        #--------------------------------------------------#\n",
    "        # compute reach\n",
    "        #--------------------------------------------------#\n",
    "        output_buffer = mc_reach.compute_reach(boundary, previous_state, parameter_inputs, output_buffer, assume_short_ts)\n",
    "\n",
    "        #--------------------------------------------------#\n",
    "        # update flowveldepth with computation results\n",
    "        #--------------------------------------------------#\n",
    "        flowveldepth.loc[reach,timestep] = np.asarray(output_buffer).tolist()\n",
    "\n",
    "        timestep += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.51084804534912\n"
     ]
    }
   ],
   "source": [
    "from joblib import delayed, Parallel\n",
    "\n",
    "# initialize the flowveldepth array\n",
    "fdv_columns = pd.MultiIndex.from_product([range(nts), [\"q\", \"v\", \"d\"]])\n",
    "flowveldepth = pd.DataFrame(0, index=data.index, columns=fdv_columns, dtype = \"float32\")\n",
    "\n",
    "# short timestep assumption?\n",
    "assume_short_ts = True\n",
    "\n",
    "t1 = time.time()\n",
    "# loop through groups\n",
    "for i in range(max(overall_ordered_reaches_dict.keys()),-1,-1):\n",
    "            \n",
    "    for reach in overall_ordered_reaches_dict[i]:\n",
    "\n",
    "        reach_timeloop(nts, reach, rconn, flowveldepth, ql, data, q0, assume_short_ts)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2 - t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
