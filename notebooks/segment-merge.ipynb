{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import glob\n",
    "import pathlib\n",
    "\n",
    "# logic to accomodate Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    ENV_IS_CL = True\n",
    "\n",
    "    root = pathlib.Path(\"/content/t-route\").resolve()\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"-b\",\n",
    "            \"segment-merge\",\n",
    "            \"https://github.com/awlostowski-noaa/t-route.git\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ! pip install geopandas\n",
    "    ! pip install zipfile\n",
    "    ! pip install us\n",
    "    ! pip install netcdf4\n",
    "    ! pip install progressbar\n",
    "\n",
    "except:\n",
    "    ENV_IS_CL = False\n",
    "    root = pathlib.Path(\"..\").resolve()\n",
    "\n",
    "fortran_source_dir = os.path.join(\n",
    "    root, \"src\", \"fortran_routing\", \"mc_pylink_v00\", \"MC_singleSeg_singleTS\"\n",
    ")\n",
    "sys.path.append(fortran_source_dir)\n",
    "routing_v02_dir = os.path.join(root, \"src\", \"python_routing_v02\")\n",
    "sys.path.append(routing_v02_dir)    \n",
    "sys.path.append(os.path.join(root, \"src\", \"python_framework_v02\"))\n",
    "sys.path.append(os.path.join(root, \"src\", \"python_framework_v01\"))\n",
    "\n",
    "# load needed scientific libraries\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter, deque\n",
    "from itertools import chain\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import us\n",
    "import time\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "# turn off warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# import t-route functions for network analysis\n",
    "import nhd_network_utilities_v02 as nnu\n",
    "import nhd_io\n",
    "import nhd_network\n",
    "import mc_reach\n",
    "import network_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHD data loaded, masked, and prepped\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "# Load, mask, and prep network data - Cape Fear River Basin, North Carolina\n",
    "#####################################################################################\n",
    "\n",
    "# Create directory path variable for test/input/geo, where NHD data and masks are stored\n",
    "test_folder = os.path.join(root, r\"test\")\n",
    "geo_input_folder = os.path.join(test_folder, r\"input\", r\"geo\")\n",
    "\n",
    "# Load network meta data for the Cape Fear Basin\n",
    "supernetwork = \"CapeFear_FULL_RES\"\n",
    "network_data = nnu.set_supernetwork_data(\n",
    "    supernetwork=supernetwork, geo_input_folder=geo_input_folder\n",
    ")\n",
    "\n",
    "# if the NHDPlus RouteLink file does not exist, download it.\n",
    "if not os.path.exists(network_data[\"geo_file_path\"]):\n",
    "    filename = os.path.basename(network_data[\"geo_file_path\"])\n",
    "    network_dl.download(network_data[\"geo_file_path\"], network_data[\"data_link\"])\n",
    "\n",
    "# read-in NHD data, retain copies for viz- and full network analysis purposes\n",
    "data = nhd_io.read(network_data[\"geo_file_path\"])\n",
    "data_full = data.copy()\n",
    "dat_geo = data.copy()\n",
    "\n",
    "# select only the necessary columns needed for network analysis and computation\n",
    "cols = [v for c, v in network_data.items() if c.endswith(\"_col\")]\n",
    "data = data[cols]\n",
    "\n",
    "# set the Data Frame index (row labels) to the key column - \"featureID\"\n",
    "data = data.set_index(network_data[\"key_col\"])\n",
    "dat_geo = dat_geo.set_index(network_data[\"key_col\"])\n",
    "\n",
    "# mask NHDNetwork to isolate test network - full resolution Cape Fear basin, NC\n",
    "if \"mask_file_path\" in network_data:\n",
    "    data_mask = nhd_io.read_mask(\n",
    "        network_data[\"mask_file_path\"], layer_string=network_data[\"mask_layer_string\"],\n",
    "    )\n",
    "    data = data.filter(data_mask.iloc[:, network_data[\"mask_key\"]], axis=0)\n",
    "    dat_geo = dat_geo.filter(data_mask.iloc[:, network_data[\"mask_key\"]], axis=0)\n",
    "\n",
    "# sort data by index\n",
    "data = data.sort_index()\n",
    "\n",
    "# replace downstreams for terminal segments\n",
    "data = nhd_io.replace_downstreams(data, network_data[\"downstream_col\"], 0)\n",
    "\n",
    "print(\"NHD data loaded, masked, and prepped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Visualize the Cape Fear basin\n",
    "#####################################################################################\n",
    "\n",
    "# create a pandas GeoDataFrame for viz purposes\n",
    "gdf = gpd.GeoDataFrame(dat_geo, geometry=gpd.points_from_xy(dat_geo.lon, dat_geo.lat))\n",
    "\n",
    "################################################\n",
    "# Download North Carolina shapefile for basemap\n",
    "################################################\n",
    "shape_url = us.states.NC.shapefile_urls()[\"state\"]\n",
    "local_path = \"../test/input/geo/NorthCarolina_state_shape\"\n",
    "\n",
    "# Download and extract shapefile\n",
    "r = requests.get(shape_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(path=local_path)\n",
    "\n",
    "# get extracted filenames\n",
    "filenames = [\n",
    "    y\n",
    "    for y in sorted(z.namelist())\n",
    "    for ending in [\"dbf\", \"prj\", \"shp\", \"shx\"]\n",
    "    if y.endswith(ending)\n",
    "]\n",
    "dbf, prj, shp, shx = [filename for filename in filenames]\n",
    "\n",
    "# load shapefile with GeoPandas\n",
    "nc = gpd.read_file(local_path + \"/\" + shp)\n",
    "\n",
    "################################################\n",
    "# Create a map\n",
    "################################################\n",
    "\n",
    "# create a basemap\n",
    "base = nc.plot(color=\"white\", edgecolor=\"black\")\n",
    "\n",
    "# add our stream network, segments are represented as small points (nodes in the network)\n",
    "gdf.plot(ax=base, markersize=0.01, color=\"black\")\n",
    "\n",
    "# crop the map extent\n",
    "base.set_xlim(-85, -75)\n",
    "base.set_ylim(33, 37)\n",
    "\n",
    "# turn axes off\n",
    "base.axis(\"off\")\n",
    "\n",
    "# get current figure and set size\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 15)\n",
    "\n",
    "# add a title and display\n",
    "plt.title(\"Cape Fear River Basin - Full Resolution NHD\", fontsize=20)\n",
    "\n",
    "# display the map in output\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/cape_fear_network.png\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Analyze the distribution of segment lengths in the native NHD Full Res data\n",
    "#####################################################################################\n",
    "\n",
    "################################################\n",
    "# Create a histogram of segment lengths\n",
    "################################################\n",
    "\n",
    "# Cape Fear Basin\n",
    "data.Length.hist(bins=100)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.set_xlabel(\"Segment Length (m)\", size=14)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# CONUS NHD Full Resolution\n",
    "data_full.Length.hist(bins=500)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.set_xlabel(\"Segment Length (m)\", size=14)\n",
    "ax.set_title(\"CONUS NHDPlus Full Res\")\n",
    "plt.xlim([0,14000])\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/nhd_fullres_conus_length_distribution.png\", bbox_inches='tight')\n",
    "\n",
    "################################################\n",
    "# What fraction of segments on the network are\n",
    "# shorter than specified threshold?\n",
    "################################################\n",
    "\n",
    "# specify threshold length\n",
    "thresh = 1000  # meters\n",
    "\n",
    "short_segs = len(data_full[data_full.Length <= thresh].index)\n",
    "total_segs = len(data_full.index)\n",
    "\n",
    "print(\n",
    "    \"There are %s segments in the supernetwork shorter than the threshold length.\"\n",
    "    % (short_segs)\n",
    ")\n",
    "print(\n",
    "    \"That is %s%% of all segments in the supernetwork!\"\n",
    "    % round(short_segs / total_segs * 100)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_connections(data, network_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract upstream and downstream connections between segments in network\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Network parameter dataset, prepared\n",
    "        network_data (dict): network metadata\n",
    "\n",
    "    Returns:\n",
    "        conn (dict): downstream connections\n",
    "        rconn (dict): upstream connections \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract downstream connections\n",
    "    conn = nhd_network.extract_connections(data, network_data[\"downstream_col\"])\n",
    "    \n",
    "    # extract upstream connections\n",
    "    rconn = nhd_network.reverse_network(rconn)\n",
    "    \n",
    "    return conn, rconn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Extract network connections\n",
    "#####################################################################################\n",
    "\n",
    "conn, rconn = extract_connections(data, network_data)\n",
    "\n",
    "# # extract the downstream connections of each segment\n",
    "# connections = nhd_network.extract_connections(data, network_data[\"downstream_col\"])\n",
    "\n",
    "# # reverse the network - extract the upstream connections of each segment\n",
    "# rconn = nhd_network.reverse_network(connections)\n",
    "\n",
    "print(\"Network connections extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headwater_connections(connections, rconn):\n",
    "\n",
    "    \"\"\"\n",
    "    Determine which segments are and are not headwaters. \n",
    "    Headwaters are defined as segments with no upstream connection, only downstream connections. \n",
    "    Non-headwaters are defined as segments with both upstream and downstream connections. \n",
    "\n",
    "    Args:\n",
    "        connections (dict): downstream connections\n",
    "        rconn (dict): upstream connections\n",
    "\n",
    "    Returns:\n",
    "        hw_connections (dict): downstream connections, headwaters only\n",
    "        non_hw_connections (dict): downstream connections, non headwaters only\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    hw = []  # store headwater segments\n",
    "    non_hw = []  # store non-headwater segments\n",
    "\n",
    "    for seg in rconn.keys():\n",
    "        # if there is no upstream connection anda downstream connection, it is a headwater\n",
    "        if bool(rconn[seg]) == False and bool(connections[seg]) == True:\n",
    "            hw.append(seg)\n",
    "\n",
    "        # if there is an upstream connection and a downstream connection, it is a non-headwater (midwater?)\n",
    "        elif bool(rconn[seg]) == True and bool(connections[seg]) == True:\n",
    "            non_hw.append(seg)\n",
    "\n",
    "    # get segment key-value pairs from the connections dictionary\n",
    "    hw_connections = {key: connections[key] for key in hw}\n",
    "    non_hw_connections = {key: connections[key] for key in non_hw}\n",
    "\n",
    "    return hw_connections, non_hw_connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_headwaters(chop, hw_connections, non_hw_connections, data, thresh):\n",
    "\n",
    "    \"\"\"\n",
    "    Prune headwaters from the network\n",
    "    Headwaters are pruned if they are shorter than the threshold length\n",
    "    OR if they merge with a midwater that is less than the threshold length\n",
    "\n",
    "    Args:\n",
    "        chop (list): A list of pruned headwater indices\n",
    "        hw_connections (dict): downstream connections, headwaters only\n",
    "        non_hw_connections (dict): downstream connections, midwaters only\n",
    "        data (DataFrame): Network to be pruned\n",
    "        thresh (int): length threshold, segments below this length are slated for pruning\n",
    "\n",
    "    Returns:\n",
    "        chop (list): list of pruned headwater indices, updated\n",
    "        data_pruned (DataFrame): pruned network\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # loop through keys and values in headwater connections dictionary\n",
    "    for hw_k, srch_v in hw_connections.items():\n",
    "\n",
    "        \"\"\"\n",
    "        Trim headwater-headwater junctions\n",
    "        \n",
    "        - Headwater-headwater junctions are junctions where two (or more) headwaters merge. \n",
    "        - Once headwater-headwater junctions are identified, the shortest headwater below the\n",
    "          threshold length is pruned. \n",
    "        \"\"\"\n",
    "\n",
    "        # create a list of headwaters draining to the specified downstream connections, srch_v\n",
    "        G = [k for k, v in hw_connections.items() if v == srch_v]\n",
    "\n",
    "        # if more than one headwater drains into the segment, it is a headwater-headwater junction\n",
    "        if len(G) > 1:\n",
    "\n",
    "            # get segment lengths from the test dataset\n",
    "            hw_len = data.loc[G].Length\n",
    "\n",
    "            # find the shortest segment\n",
    "            hw_min_len = hw_len[hw_len.idxmin()]\n",
    "\n",
    "            # check to see if headwater shorter than threshold length\n",
    "            if hw_min_len < thresh:\n",
    "\n",
    "                # update chopping block\n",
    "                chop.append(hw_len.idxmin())\n",
    "\n",
    "        ############################################################\n",
    "        #  Trim headwater-midwater junctions\n",
    "        ############################################################\n",
    "\n",
    "        \"\"\"\n",
    "        Trim headwater-midwater junctions\n",
    "        \n",
    "        - Headwater-midwater junctions are junctions a headwater merges with a non-headwater (i.e. midwater). \n",
    "        - If the headwater is shorter than the threshold, it is pruned.\n",
    "        - If the headwater is longer than the threshold, but joins with or connects to a midwater that is\n",
    "          shorter than the threshold, the headwater is pruned. \n",
    "              - This is done to minimize the number of small midwater segments stranded between headwaters.\n",
    "                Else, these short, stranded segments cannot be merged upstream or downstream. \n",
    "        \"\"\"\n",
    "\n",
    "        # midwaters draining into a junction that a headwater does too\n",
    "        H = [k for k, v in non_hw_connections.items() if v == srch_v]\n",
    "\n",
    "        # if there is a corresponding midwater, then the headwater is a candidate for trimming\n",
    "        if bool(H) == True:\n",
    "\n",
    "            # get segment lengths from the dataset\n",
    "            hw_len = data.loc[hw_k].Length\n",
    "\n",
    "            # get midwater (in) length from the test dataset\n",
    "            mw_in_len = data.loc[H[0]].Length\n",
    "\n",
    "            # get midwater (out) length from the test dataset\n",
    "            mw_out_len = data.loc[srch_v[0]].Length\n",
    "\n",
    "            # check to see if headwater or midwaters (in/out) are shorter than the threshold\n",
    "            if hw_len < thresh or mw_in_len < thresh or mw_out_len < thresh:\n",
    "\n",
    "                # update chopping block\n",
    "                chop.append(hw_k)\n",
    "\n",
    "    # chop segments from the network\n",
    "    data_pruned = data.drop(np.unique(chop))\n",
    "\n",
    "    return np.unique(chop), data_pruned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Prune headwaters\n",
    "#####################################################################################\n",
    "\n",
    "# identify headwater and non-headwater (midwater) reaches (that also have a downstream connection)\n",
    "hw_connections, non_hw_connections = headwater_connections(connections, rconn)\n",
    "\n",
    "# Prune the network\n",
    "print(\"Pruning headwaters...\")\n",
    "\n",
    "pruned_headwaters = []\n",
    "pruned_headwaters, data_pruned = prune_headwaters(\n",
    "    pruned_headwaters, hw_connections, non_hw_connections, data, thresh\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Done pruning! Removed\",\n",
    "    len(pruned_headwaters),\n",
    "    \"headwater segments, or\",\n",
    "    int(data.loc[pruned_headwaters].Length.sum() / 1000),\n",
    "    \" river km, from the network.\",\n",
    ")\n",
    "print(\n",
    "    \"That is\",\n",
    "    int((data.loc[pruned_headwaters].Length.sum() / data.Length.sum()) * 100),\n",
    "    \"% of the total network length!\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Recompute connections and construct reaches from the pruned network data\n",
    "#####################################################################################\n",
    "\n",
    "# extract downstream connections for each node\n",
    "connections_pruned = nhd_network.extract_connections(\n",
    "    data_pruned, network_data[\"downstream_col\"]\n",
    ")\n",
    "\n",
    "# reverse the network - track upstream connections\n",
    "rconn_pruned = nhd_network.reverse_network(connections_pruned)\n",
    "\n",
    "# isolate independent subnetworks\n",
    "subnets = nhd_network.reachable_network(rconn_pruned)\n",
    "\n",
    "# identify the segments in each subnetwork\n",
    "subreachable = nhd_network.reachable(rconn_pruned)\n",
    "\n",
    "# break each subnetwork into reaches\n",
    "subreaches = {}\n",
    "for tw, net in subnets.items():\n",
    "    path_func = partial(nhd_network.split_at_junction, net)\n",
    "    subreaches[tw] = nhd_network.dfs_decomposition(net, path_func)\n",
    "\n",
    "print(\"Network connections extracted and reaches created from pruned network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_weighted_av(df, var, weight):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate a weighted average\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing variables to be averaged and used as weights\n",
    "        var (str): name of the variable to be averaged\n",
    "        weight (str): name of the variable to be used as a weight\n",
    "\n",
    "    Returns:\n",
    "        x (float32): weighted average\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x = (df[weight] * df[var]).sum() / df[weight].sum()\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parameters(to_merge):\n",
    "\n",
    "    \"\"\"\n",
    "    length-weighted averaging of channel routing parameters across merged segments\n",
    "\n",
    "    Args:\n",
    "        to_merge (DataFrame): DataFrame containing routing parameters for segments to be merged together\n",
    "\n",
    "    Returns:\n",
    "        replace (DataFrame): weighted average\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data_replace = to_merge.tail(1)\n",
    "\n",
    "    idx = to_merge.tail(1).index\n",
    "    data_replace.loc[idx, \"Length\"] = to_merge.Length.sum()\n",
    "    data_replace.loc[idx, \"n\"] = len_weighted_av(to_merge, \"n\", \"Length\")\n",
    "    data_replace.loc[idx, \"nCC\"] = len_weighted_av(to_merge, \"nCC\", \"Length\")\n",
    "    data_replace.loc[idx, \"So\"] = len_weighted_av(to_merge, \"So\", \"Length\")\n",
    "    data_replace.loc[idx, \"BtmWdth\"] = len_weighted_av(to_merge, \"BtmWdth\", \"Length\")\n",
    "    data_replace.loc[idx, \"TopWdth\"] = len_weighted_av(to_merge, \"TopWdth\", \"Length\")\n",
    "    data_replace.loc[idx, \"TopWdthCC\"] = len_weighted_av(\n",
    "        to_merge, \"TopWdthCC\", \"Length\"\n",
    "    )\n",
    "    data_replace.loc[idx, \"MusK\"] = len_weighted_av(to_merge, \"MusK\", \"Length\")\n",
    "    data_replace.loc[idx, \"MusX\"] = len_weighted_av(to_merge, \"MusX\", \"Length\")\n",
    "    data_replace.loc[idx, \"ChSlp\"] = len_weighted_av(to_merge, \"ChSlp\", \"Length\")\n",
    "\n",
    "    return data_replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_reach_connections(data_merged):\n",
    "\n",
    "    \"\"\"\n",
    "    Update downstream connections (\"to\") for segments in a merged reach.\n",
    "    Only updates *in-reach* connections.\n",
    "\n",
    "    Args:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach\n",
    "\n",
    "    Returns:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach with updated donwstream connections\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for i, idx in enumerate(data_merged.index.values[0:-1]):\n",
    "        data_merged.loc[idx, \"to\"] = data_merged.index.values[i + 1]\n",
    "\n",
    "    return data_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upstream_merge(data_merged, chop):\n",
    "\n",
    "    \"\"\"\n",
    "    Merge a short reach tail segment with upstream neighbor\n",
    "\n",
    "    Args:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach\n",
    "        chop (list): list of merged-out segments\n",
    "\n",
    "    Returns:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach with updated donwstream connections\n",
    "        chop (list): updated list of merged-out segments\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # grab the two segments that need to be merged - simply the last two segments of the reach\n",
    "    to_merge = data_merged.tail(2)\n",
    "\n",
    "    # calculate new parameter values\n",
    "    data_replace = merge_parameters(to_merge)\n",
    "\n",
    "    # paste new parameters in to data_merged\n",
    "    data_merged.loc[to_merge.tail(1).index] = data_replace\n",
    "\n",
    "    # remove merged segments from data_merged\n",
    "    data_merged = data_merged.drop(to_merge.head(1).index)\n",
    "\n",
    "    # update \"chop\" list with merged-out segment IDs\n",
    "    chop.append(to_merge.head(1).index.values[0])\n",
    "\n",
    "    return data_merged, chop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downstream_merge(data_merged, chop, thresh):\n",
    "\n",
    "    \"\"\"\n",
    "    Merge short segments with their downstream neighbors\n",
    "\n",
    "    Args:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach\n",
    "        chop (list): list of merged-out segments\n",
    "        thresh (int): theshold reach length (meters)\n",
    "\n",
    "    Returns:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach with updated donwstream connections\n",
    "        chop (list): updated list of merged-out segments\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # find the upstream-most short segment and it's downstream connection\n",
    "    idx_us = data_merged.loc[data_merged.Length < thresh].head(1).index.values[0]\n",
    "\n",
    "    pos_idx_us = data_merged.index.get_loc(idx_us)\n",
    "    idx_to = data_merged.iloc[pos_idx_us + 1].name\n",
    "\n",
    "    # grab segments to be merged\n",
    "    to_merge = data_merged.loc[[idx_us, idx_to]]\n",
    "\n",
    "    # calculate new parameter values\n",
    "    data_replace = merge_parameters(to_merge)\n",
    "\n",
    "    # paste new parameters in to data_merged\n",
    "    data_merged.loc[to_merge.tail(1).index] = data_replace\n",
    "\n",
    "    # remove merged segments from data_merged\n",
    "    data_merged = data_merged.drop(to_merge.head(1).index)\n",
    "\n",
    "    # update \"chop\" list with merged-out segment IDs\n",
    "    chop.append(to_merge.head(1).index.values[0])\n",
    "\n",
    "    return data_merged, chop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all(rch, data, chop):\n",
    "\n",
    "    \"\"\"\n",
    "    Merge all segments in a reach\n",
    "\n",
    "    Args:\n",
    "        rch (list): Segment indices in the reach to be merged\n",
    "        data (DataFrame): Routing parameters for network containing the reach to be merged\n",
    "        chop (list): list of merged-out segments\n",
    "\n",
    "    Returns:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach with updated donwstream connections\n",
    "        chop (list): updated list of merged-out segments\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # subset the model parameter data for this reach\n",
    "    data_merged = data.loc[rch].copy()\n",
    "\n",
    "    # grab the two segments that need to be merged - in this case, merge all segments!\n",
    "    to_merge = data_merged.copy()\n",
    "\n",
    "    # calculate new parameter values\n",
    "    data_replace = merge_parameters(to_merge)\n",
    "\n",
    "    # paste new parameters in to data_merged\n",
    "    data_merged.loc[to_merge.tail(1).index] = data_replace\n",
    "\n",
    "    # remove merged segments from data_merged - in this case, all but the last\n",
    "    data_merged = data_merged.drop(data_merged.iloc[:-1, :].index)\n",
    "\n",
    "    # update \"chop\" list with merged-out segment IDs\n",
    "    chop.extend(list(to_merge.iloc[:-1, :].index))\n",
    "\n",
    "    return data_merged, chop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_length(data, data_merged):\n",
    "\n",
    "    \"\"\"\n",
    "    Check the merged network to the total stream length has not changed.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Routing parameters for pre-merged network\n",
    "        data_merged (DataFrame): Routing parameters for merged reach\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if data.Length.sum() != data_merged.Length.sum():\n",
    "        print(\"Error: The merging process has changed the network length\")\n",
    "    else:\n",
    "        print(\"Good stuff! The merging process did not change the network length!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network_data(data, rch, data_merged, chop, rconn):\n",
    "\n",
    "    \"\"\"\n",
    "    Update the network routing parameter data with merged segment data\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Routing parameters for network to be updated\n",
    "        rch (list): Segment indices in the reach to be merged\n",
    "        data_merged (DataFrame): Routing parameters for merged reach\n",
    "\n",
    "    Returns:\n",
    "        data (DataFrame): Updated network routing parameters\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # make a copy of the data to be replaced with merged\n",
    "    data_old = data.loc[rch].copy()\n",
    "\n",
    "    # drop the segments that disapeared with merger\n",
    "    data = data.drop(chop)\n",
    "\n",
    "    # adjust the segment data for those that remain\n",
    "    data.loc[data_merged.index] = data_merged\n",
    "\n",
    "    # update out of reach connections - these will change in the first segment was merged out\n",
    "    upstreams = rconn[\n",
    "        data_old.head(1).index.values[0]\n",
    "    ]  # upstream connection of the OLD reach head\n",
    "\n",
    "    if bool(upstreams):\n",
    "        \n",
    "        data.loc[upstreams, \"to\"] = data_merged.head(1).index.values[\n",
    "            0\n",
    "        ]  # index of NEW reach head\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_network(data, data_merged, network_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Check the merged network to ensure subnetwork structure has not changed.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Routing parameters for pre-merged network\n",
    "        data_merged (DataFrame): Routing parameters for merged network\n",
    "        network_data (dict): supernetwork meta data containing the column name of the downstream connection column\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # extract downstream connections for each node\n",
    "    connections_merged = nhd_network.extract_connections(\n",
    "        data_merged, network_data[\"downstream_col\"]\n",
    "    )\n",
    "    connections = nhd_network.extract_connections(data, network_data[\"downstream_col\"])\n",
    "\n",
    "    # reverse the network - track upstream connections\n",
    "    rconn_merged = nhd_network.reverse_network(connections_merged)\n",
    "    rconn = nhd_network.reverse_network(connections)\n",
    "\n",
    "    # isolate independent subnetworks\n",
    "    subnets_merged = nhd_network.reachable_network(rconn_merged)\n",
    "    subnets = nhd_network.reachable_network(rconn)\n",
    "\n",
    "    # compare the number of tailwaters\n",
    "    if len(subnets_merged.keys()) != len(subnets.keys()):\n",
    "        print(\"Error: merging process has changed number of subnetworks!\")\n",
    "    else:\n",
    "        print(\"Good stuff! The merging process has preserved the number of subnetworks\")\n",
    "        \n",
    "    # test network connectivity\n",
    "    # start at each headwater and trace it down to the tailwater by consecutively finding the \"to\" segment.\n",
    "    # if the \"to\" segment cannot be found, there is a discontinuity in the network\n",
    "    \n",
    "    hw_connections, non_hw_connections = headwater_connections(connections_merged, rconn_merged)\n",
    "    \n",
    "    print(\"checking to make sure downstream connections are continuous...\")\n",
    "\n",
    "    pbar = ProgressBar()\n",
    "    for hw_k in pbar(hw_connections.keys()):\n",
    "        \n",
    "        err_flg = 0\n",
    "        ds_old = hw_k\n",
    "        ds = int(data_merged.loc[hw_k].to)\n",
    "        \n",
    "        while (ds > 0):\n",
    "            if bool(ds in data_merged.index) == True:\n",
    "                ds_old = ds\n",
    "                ds = int(data_merged.loc[ds].to)\n",
    "            else:\n",
    "                print(\"Error: The network is discontinuous!\")\n",
    "                print(\"Segment\",ds_old,\"has no downstream connection.\")\n",
    "                print(\"It should be connected to\", ds, \", which doesn't exist in the pruned/merged network.\")\n",
    "                err_flg = 1\n",
    "                break\n",
    "                \n",
    "        if err_flg == 1:\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    if err_flg == 0:      \n",
    "        print(\"Done checking connections - pruned/merge network is continuous from headwaters to tailwater(s)\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlat_destination_compute(data_native, data_merged, merged_segments, pruned_segments, network_data):\n",
    "    \n",
    "    if bool(list(pruned_segments)):\n",
    "        \n",
    "        segments = merged_segments + list(pruned_segments)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        segments = merged_segments\n",
    "\n",
    "    # compute connections\n",
    "    conn = nhd_network.extract_connections(data_native, network_data[\"downstream_col\"])\n",
    "    \n",
    "    # initialize a library to store qlat desinations for pruned/merged segments\n",
    "    qlat_destinations = {x: {} for x in segments}\n",
    "\n",
    "    for idx in segments:\n",
    "\n",
    "        # get the downstream connection of this segment in the original network\n",
    "        ds_idx = conn[idx]\n",
    "\n",
    "        # find the nearest downstream segment remaining in the pruned/merged network \n",
    "        while bool(ds_idx[0] in data_merged.index) == False:\n",
    "            ds_idx = conn[ds_idx[0]]\n",
    "\n",
    "        # update the qlat destination dict\n",
    "        qlat_destinations[idx] = ds_idx\n",
    "        \n",
    "    return qlat_destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_merge(data_native, data, network_data, thresh, pruned_segments):\n",
    "    \n",
    "    # create a copy of the pruned network dataset, which will be updated with merged data\n",
    "    data_merged = data.copy()\n",
    "\n",
    "    # initialize list to store merged segment IDs\n",
    "    merged_segments = []\n",
    "    \n",
    "    # organize network into reaches\n",
    "    conn = nhd_network.extract_connections(data, network_data[\"downstream_col\"])\n",
    "    rconn = nhd_network.reverse_network(conn)\n",
    "    subnets = nhd_network.reachable_network(rconn)\n",
    "    subreachable = nhd_network.reachable(rconn)\n",
    "    \n",
    "    subreaches = {}\n",
    "    for tw, net in subnets.items():\n",
    "        path_func = partial(nhd_network.split_at_junction, net)\n",
    "        subreaches[tw] = nhd_network.dfs_decomposition(net, path_func)\n",
    "    \n",
    "    # loop through each reach in the network\n",
    "    for twi, (tw, rchs) in enumerate(subreaches.items(), 1):\n",
    "\n",
    "        for rch in rchs:\n",
    "\n",
    "            # calculate reach length\n",
    "            rch_len = data.loc[rch].Length.sum()\n",
    "\n",
    "            ##################################################\n",
    "            # orphaned short single segment reaches\n",
    "            ##################################################\n",
    "            # if reach length is shorter than threshold and composed of a single segment\n",
    "            if rch_len < thresh and len(data.loc[rch]) == 1:\n",
    "                continue  # do nothing\n",
    "\n",
    "            ##################################################\n",
    "            # multi segment reaches - combine into a single segment reach\n",
    "            ##################################################\n",
    "            # if reach length is shorter than threshold and composed more than one segment\n",
    "            if rch_len < thresh and len(data.loc[rch]) > 1:\n",
    "\n",
    "                # merge ALL reach segments into one\n",
    "                chop = []\n",
    "                reach_merged, chop = merge_all(rch, data, chop)\n",
    "\n",
    "                # update network with merged reach data\n",
    "                data_merged = update_network_data(\n",
    "                    data_merged, \n",
    "                    rch, \n",
    "                    reach_merged, \n",
    "                    chop, \n",
    "                    rconn\n",
    "                )\n",
    "\n",
    "                # update merged_segmetns list with merged-out segments\n",
    "                merged_segments.extend(chop)\n",
    "\n",
    "            ##################################################\n",
    "            # multi segment reaches longer than threshold with some segments shorter than threshold\n",
    "            ##################################################\n",
    "            # if reach length is longer than threshold and smallest segment length is less than threshold\n",
    "            if rch_len > thresh and data.loc[rch].Length.min() < thresh:\n",
    "\n",
    "                # initialize data_merged - this DataFrame will be subsequently revised\n",
    "                reach_merged = data.loc[rch]\n",
    "\n",
    "                # initialize list of segments chopped from this reach\n",
    "                chop_reach = []\n",
    "\n",
    "                # so long as the shortest segment is shorter than the threshold...\n",
    "                while reach_merged.Length.min() < thresh:\n",
    "\n",
    "                    # if shortest segment is the last segment in the reach - conduct an upstream merge.\n",
    "                    if (\n",
    "                        reach_merged.Length.idxmin() == reach_merged.tail(1).index.values[0]\n",
    "                        and reach_merged.Length.min() < thresh\n",
    "                    ):\n",
    "\n",
    "                        # upstream merge\n",
    "                        chop = []\n",
    "                        reach_merged, chop = upstream_merge(reach_merged, chop)\n",
    "\n",
    "                        # update chop_reach list with merged-out segments\n",
    "                        chop_reach.extend(chop)\n",
    "\n",
    "                    # if shortest segment is NOT the last segment in the reach - conduct a downstream merge\n",
    "                    if (\n",
    "                        reach_merged.Length.idxmin() != reach_merged.tail(1).index.values[0]\n",
    "                        and reach_merged.Length.min() < thresh\n",
    "                    ):\n",
    "\n",
    "                        # downstream merge\n",
    "                        chop = []\n",
    "                        reach_merged, chop = downstream_merge(reach_merged, chop, thresh)\n",
    "\n",
    "                        # update chop_reach list with merged-out segments\n",
    "                        chop_reach.extend(chop)\n",
    "\n",
    "                # correct segment connections within reach\n",
    "                reach_merged = correct_reach_connections(reach_merged)\n",
    "\n",
    "                # update the greater network data set\n",
    "                data_merged = update_network_data(\n",
    "                    data_merged,\n",
    "                    rch,\n",
    "                    reach_merged,\n",
    "                    chop_reach,\n",
    "                    rconn\n",
    "                )\n",
    "\n",
    "                # update merged_segmetns list with merged-out segments\n",
    "                merged_segments.extend(chop_reach)\n",
    "                \n",
    "    # create a qlateral destinations dictionary\n",
    "    qlat_destinations = qlat_destination_compute(data_native,\n",
    "                                                 data_merged,\n",
    "                                                 merged_segments,\n",
    "                                                 pruned_segments, \n",
    "                                                 network_data)\n",
    "    \n",
    "    return data_merged, qlat_destinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Merge-out short segments\n",
    "#####################################################################################\n",
    "\n",
    "##################################################\n",
    "# Merge segments in native network\n",
    "##################################################\n",
    "print(\"Merging segments in native NHDPlus Full Res data...\")\n",
    "ts = time.time()\n",
    "data_merged, qlat_dest_merged = segment_merge(data,\n",
    "                                              data,\n",
    "                                              network_data,\n",
    "                                              thresh,\n",
    "                                              pruned_segments = [])\n",
    "tf = time.time()\n",
    "print(\"Merging complete, that took\", round((tf - ts)/60, 2), \"minutes\")\n",
    "\n",
    "##################################################\n",
    "# Merge segents in pruned network\n",
    "##################################################\n",
    "print(\"Merging segments in pruned NHDPlus Full Res data...\")\n",
    "ts = time.time()\n",
    "data_pruned_merged, qlat_dest_pruned_merged = segment_merge(data,\n",
    "                                                            data_pruned,\n",
    "                                                            network_data,\n",
    "                                                            thresh,\n",
    "                                                            pruned_segments = pruned_headwaters)\n",
    "tf = time.time()\n",
    "print(\"Merging complete, that took\", round((tf - ts)/60, 2), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# How did pruning and merging change the distribution of segment lengths?\n",
    "#####################################################################################\n",
    "\n",
    "##################################################\n",
    "data.Length.hist(bins=100)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.set_xlabel(\"Segment Length (m)\", size=14)\n",
    "plt.ylim = [0,2500]\n",
    "plt.title(\"native network\", fontsize=20)\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/capefear_native_length_distribution.png\", bbox_inches='tight')\n",
    "\n",
    "##################################################\n",
    "data_merged.Length.hist(bins=100)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.set_xlabel(\"Segment Length (m)\", size=14)\n",
    "plt.title(\"Merged network\", fontsize=20)\n",
    "\n",
    "plt.ylim = [0,2500]\n",
    "fig = plt.gcf()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/capefear_merged_length_distribution.png\", bbox_inches='tight')\n",
    "\n",
    "##################################################\n",
    "data_pruned_merged.Length.hist(bins=100)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.set_xlabel(\"Segment Length (m)\", size=14)\n",
    "plt.ylim = [0,2500]\n",
    "plt.title(\"pruned and merged network\", fontsize=20)\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/capefear_pruned_merged_length_distribution.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps: Questions\n",
    "- How does the pruned/merged network affect routing computations?\n",
    "- Does it speed up simulation times? \n",
    "- Does it change the timing and shape of main-stem hydrographs?\n",
    "- How are routing differences (btw native and pruned/merged networks) sensitive to:\n",
    "    - threshold lengths\n",
    "    - the position in the network where differences are assessed (e.g. 3rd v. 5th order rivers)?\n",
    "    \n",
    "# Next Steps: Actions\n",
    "- Simulate routing of artificial boundary conditions (qlats) with MC on native NHD network. \n",
    "- Simulate routing of same boundary conditions with pruned/merged network. \n",
    "- Check that the pruned/merged network maintains mass balance. \n",
    "- Use an objective function (e.g. RMSE) to compare simulations between native and pruned/merged network\n",
    "- Compare simulation run-times\n",
    "- Test sensitivity of threshold length. Expect progressive decline in run time and performance as threshold length increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = {\"Southport\": 8835402,\n",
    "         \"Wilmington\": 10529081,\n",
    "         \"Elizabethtown\": 8831808,\n",
    "         \"Lillington\": 8846907}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of simulated timesteps\n",
    "nts = 3600\n",
    "\n",
    "# duration of each timestep (seconds)\n",
    "dt = 3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_qlats(data, nsteps, qlat):\n",
    "\n",
    "    ##################################################\n",
    "    # Create lateral inflow series, repeat on every \n",
    "    # segment in the network\n",
    "    ##################################################\n",
    "        \n",
    "    q1 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q2 = np.full((len(data.index), nsteps // 10), qlat, dtype=\"float32\")\n",
    "    q3 = np.full((len(data.index), nsteps // 10), qlat, dtype=\"float32\")\n",
    "    q4 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q5 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q6 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q7 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q8 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q9 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "    q10 = np.full((len(data.index), nsteps // 10), 0, dtype=\"float32\")\n",
    "\n",
    "    q = np.concatenate((q1, q2, q3, q4, q5, q6, q7, q8, q9, q10), axis=1)\n",
    "\n",
    "    ql = pd.DataFrame(q, index=data.index, columns=range(nsteps))\n",
    "    \n",
    "    return ql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlat_merger_adjust(ql, ql_destinations):\n",
    "    \n",
    "    ql_merged = ql.copy()\n",
    "    \n",
    "    for i, (seg, dest) in enumerate(ql_destinations.items()):\n",
    "        \n",
    "        # add lateral inflows from pruned/merged segment to downstream destination\n",
    "        series = ql_merged.loc[[seg, dest[0]]].sum(axis = 0)\n",
    "        series.name = dest[0]\n",
    "        \n",
    "        ql_merged.loc[dest] = pd.DataFrame(series).T\n",
    "     \n",
    "    # drop pruned/merged segments\n",
    "    ql_merged = ql_merged.drop(ql_destinations.keys())\n",
    "    \n",
    "    return ql_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the native lateral inflow data\n",
    "qlat_native = step_qlats(data, nts, 10.0)\n",
    "\n",
    "# adjust lateral inflows data to account for pruning and merging\n",
    "qlat_merged = qlat_merger_adjust(qlat_native, qlat_dest_merged)\n",
    "qlat_pruned_merged = qlat_merger_adjust(qlat_native, qlat_dest_pruned_merged)\n",
    "\n",
    "# plot lateral inflow series\n",
    "qlat_native.iloc[1].plot(color=\"r\", linewidth=2)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_title(\"Lateral Inflow Loading\", size=20)\n",
    "ax.set_ylabel(\"Lateral Inflow (cms per segment)\", size=14)\n",
    "ax.set_xlabel(\"Simulated Timestep\", size=14)\n",
    "\n",
    "fig = plt.gcf()\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/lateral_inflow_series.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_network(tail_water, data, dt, nts, qlats, network_data):\n",
    "    \n",
    "    # compute downstream connections\n",
    "    conn = nhd_network.extract_connections(data, network_data[\"downstream_col\"])\n",
    "    \n",
    "    # compute upstream connections\n",
    "    rconn = nhd_network.reverse_network(conn)\n",
    "    \n",
    "    # break subnetwork into reaches\n",
    "    subnets = nhd_network.reachable_network(rconn)\n",
    "    subreachable = nhd_network.reachable(rconn)\n",
    "    \n",
    "    subreaches = {}\n",
    "    for tw, net in subnets.items():\n",
    "        path_func = partial(nhd_network.split_at_junction, net)\n",
    "        subreaches[tw] = nhd_network.dfs_decomposition(net, path_func)\n",
    "        \n",
    "    results = []\n",
    "    reach = subreaches[tail_water]\n",
    "    \n",
    "    r = list(filter(None, chain.from_iterable(reach)))\n",
    "    print(\"Total network length:\",data.loc[r].Length.sum())\n",
    "    \n",
    "    # add a dt column to the data DataFrame\n",
    "    data[\"dt\"] = dt\n",
    "\n",
    "    # rename columns to specific variable names expected by mc_reach.compute_network\n",
    "    column_rename = {\n",
    "        network_data[\"length_col\"]: \"dx\",\n",
    "        network_data[\"topwidth_col\"]: \"tw\",\n",
    "        network_data[\"topwidthcc_col\"]: \"twcc\",\n",
    "        network_data[\"bottomwidth_col\"]: \"bw\",\n",
    "        network_data[\"manningncc_col\"]: \"ncc\",\n",
    "        network_data[\"slope_col\"]: \"s0\",\n",
    "        network_data[\"ChSlp_col\"]: \"cs\",\n",
    "        network_data[\"manningn_col\"]: \"n\",\n",
    "    }\n",
    "\n",
    "    data = data.rename(columns=column_rename)\n",
    "\n",
    "    # change variables to type float32, as expected by mc_reach.compute_network\n",
    "    data = data.astype(\"float32\")\n",
    "\n",
    "    # prep parameter and lateral inflow data to be fed to routing model\n",
    "    data_sub = data.loc[\n",
    "        r, [\"dt\", \"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\"]\n",
    "    ].sort_index()\n",
    "    \n",
    "    # !!! something strange is happening, here\n",
    "    qlat_sub = qlats.loc[r].sort_index()\n",
    "    \n",
    "    print(\"Total lateral inflows:\",qlat_sub.sum().sum())\n",
    "\n",
    "    # compute the network routing, calculate (flow, depth, and velocity)\n",
    "    start_time = time.time()\n",
    "    results.append(\n",
    "        mc_reach.compute_network(\n",
    "            nts,\n",
    "            reach,\n",
    "            subnets[tail_water],\n",
    "            data_sub.index.values.astype(\"int64\"),\n",
    "            data_sub.columns.values,\n",
    "            data_sub.values,\n",
    "            qlat_sub.values,\n",
    "        )\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    timing = end_time - start_time\n",
    "    \n",
    "    # create a multi-index DataFrame with flow, depth, and velocity simulations\n",
    "    fdv_columns = pd.MultiIndex.from_product([range(nts), [\"q\", \"v\", \"d\"]])\n",
    "    flowveldepth = pd.concat(\n",
    "        [pd.DataFrame(d, index=i, columns=fdv_columns) for i, d in results], copy=False\n",
    "    )\n",
    "    flowveldepth = flowveldepth.sort_index()\n",
    "\n",
    "    return flowveldepth, timing, qlat_sub\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = 8835402\n",
    "\n",
    "print(\"Simulating the native network\")\n",
    "flowveldepth_native, timing, qlat_sub = simulate_network(tw, \n",
    "                                                         data, \n",
    "                                                         dt, \n",
    "                                                         nts, \n",
    "                                                         qlat_native, \n",
    "                                                         network_data)\n",
    "print(\"Simulation complete, that took\", round((timing)/60, 2), \"minutes\")\n",
    "\n",
    "print(\"Simulating the merged network\")\n",
    "flowveldepth_merged, timing, qlat_sub = simulate_network(tw, \n",
    "                                                         data_merged, \n",
    "                                                         dt, \n",
    "                                                         nts, \n",
    "                                                         qlat_merged, \n",
    "                                                         network_data)\n",
    "print(\"Simulation complete, that took\", round((timing)/60, 2), \"minutes\")\n",
    "\n",
    "print(\"Simpulating the pruned and merged network\")\n",
    "flowveldepth_pruned_merged, timing, qlat_sub_pruned_merged = simulate_network(tw, \n",
    "                                                                              data_pruned_merged, \n",
    "                                                                              dt, \n",
    "                                                                              nts, \n",
    "                                                                              qlat_pruned_merged, \n",
    "                                                                              network_data)\n",
    "print(\"Simulation complete, that took\", round((timing)/60, 2), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gif\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# find segments in the Cape Fear Basin\n",
    "tw = 8835386\n",
    "conn = nhd_network.extract_connections(data, network_data[\"downstream_col\"])\n",
    "rconn = nhd_network.reverse_network(conn)\n",
    "subreachable = nhd_network.reachable(rconn)\n",
    "network_segs = subreachable[tw]\n",
    "\n",
    "# network geodata framee\n",
    "network_gdf = gdf.loc[gdf.index.isin(network_segs)]\n",
    "network_flows = flowveldepth_native.loc[:, (slice(None), \"q\")]\n",
    "\n",
    "@gif.frame\n",
    "def plot(tstep):\n",
    "\n",
    "    first_flow = network_flows.loc[:, ((tstep), \"q\")]\n",
    "\n",
    "    joined_gdf = network_gdf.join(first_flow)\n",
    "    joined_gdf = joined_gdf.rename(columns={joined_gdf.columns[-1]: \"flow\"})\n",
    "\n",
    "    # create a figure showing spatial variations in simulated flow and the lateral flow loading\n",
    "    fig = plt.figure(constrained_layout=True)\n",
    "    gs = fig.add_gridspec(4, 3)\n",
    "    ax1 = fig.add_subplot(gs[0:2, :])  # we will make a map on ax1\n",
    "    ax2 = fig.add_subplot(gs[2:3, :])  # we will make a line plot on ax2\n",
    "\n",
    "    # need to adjust the size and position of the map colorbar\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "    # plot the spatial variation in simulated flow rate, across all stream segments in the network\n",
    "    joined_gdf.plot(\n",
    "        markersize=0.1,\n",
    "        column=\"flow\",\n",
    "        cmap=\"PuBu\",\n",
    "        legend=True,\n",
    "        vmin=0,\n",
    "        vmax=2000,\n",
    "        ax=ax1,\n",
    "        cax=cax,\n",
    "        legend_kwds={\"label\": \"Flow (cms)\", \"orientation\": \"vertical\"},\n",
    "    )\n",
    "\n",
    "    # map title and axis controls\n",
    "    ax1.set_title(\"Cape Fear River Basin\", size=20)\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    # plot the timeseris of lateral inflow loading\n",
    "    qlat_native.iloc[1][0:tstep].plot(ax=ax2, color=\"r\", linewidth=2) \n",
    "    ax2.set_xlim([0, 3600])\n",
    "    ax2.set_ylim([0, 11])\n",
    "    ax2.set_title(\"Lateral Inflow Loading\", size=20)\n",
    "    ax2.set_ylabel(\"Lateral Inflow (cms per node)\", size=14)\n",
    "    ax2.set_xlabel(\"Simulated Timestep\", size=14)\n",
    "\n",
    "    # adjsut the figure size\n",
    "    fig.set_size_inches(10, 10)\n",
    "\n",
    "frames = []\n",
    "for i in range(0, 2025, round(nts / 160)):\n",
    "\n",
    "    frame = plot(i)\n",
    "\n",
    "    frames.append(frame)\n",
    "\n",
    "if ENV_IS_CL:\n",
    "    os.chdir(\"/content/t-route/doc/\")\n",
    "    gif.save(frames, \"cape_fear.gif\", duration=10)\n",
    "\n",
    "else:\n",
    "    gif.save(frames, \"../doc/cape_fear.gif\", duration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_output(flowveldepth, sites, sim_type):\n",
    "    \n",
    "    # Subset model output and reshape dataframe\n",
    "    flows = flowveldepth.loc[sites.values(), (slice(None), \"q\")]\n",
    "    flows = flows.T.reset_index(level = [0,1])\n",
    "\n",
    "    flows.columns = flows.columns.map(str)\n",
    "    \n",
    "    flows.rename(columns = {\"level_0\": \"Timestep\", \"level_1\": \"Parameter\"}, inplace = True)\n",
    "\n",
    "    for key, value in sites.items():\n",
    "        \n",
    "        flows.rename(columns = {str(value): key}, inplace = True)\n",
    "        \n",
    "    flows = flows.melt(id_vars=[\"Timestep\"],\n",
    "                       value_vars = list(sites.keys()),\n",
    "                       var_name=\"City\", \n",
    "                       value_name=\"Flow\")\n",
    "\n",
    "    flows[\"Simulation\"] = sim_type\n",
    "    \n",
    "    return flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_native = reshape_output(flowveldepth_native, sites, sim_type = \"native\")\n",
    "flows_merged = reshape_output(flowveldepth_merged, sites, sim_type = \"merged\")\n",
    "flows_pruned_merged = reshape_output(flowveldepth_pruned_merged, sites, sim_type = \"pruned/merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for name, group in flows_native.groupby('City'):\n",
    "    group.plot(x=\"Timestep\", y='Flow', ax=ax, label=name)\n",
    "\n",
    "plt.ylabel(\"Flow (cms)\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.title(\"Native network simulation\")\n",
    "plt.xlim([0, 3600])\n",
    "\n",
    "fig = plt.gcf()\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/native_simulation_alltime.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for name, group in flows_pruned_merged.groupby('City'):\n",
    "    group.plot(x=\"Timestep\", y='Flow', ax=ax, label=name)\n",
    "\n",
    "plt.ylabel(\"Flow (cms)\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.title(\"Pruned/Merged network simulation\")\n",
    "plt.xlim([0, 3500])\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/pruned_merged_simulation_alltime.png\", bbox_inches='tight')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for name, group in flows_merged.groupby('City'):\n",
    "    group.plot(x=\"Timestep\", y='Flow', ax=ax, label=name)\n",
    "\n",
    "plt.ylabel(\"Flow (cms)\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.title(\"Merged network simulation\")\n",
    "plt.xlim([0, 3500])\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/merged_simulation_alltime.png\", bbox_inches='tight')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for name, group in flows_native.groupby('City'):\n",
    "    group.plot(x=\"Timestep\", y='Flow', ax=ax, label=name)\n",
    "\n",
    "plt.ylabel(\"Flow (cms)\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.title(\"Native network simulation\")\n",
    "plt.xlim([0, 3500])\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/native_simulation_alltime.png\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_merged.groupby('City').Flow.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_native.groupby('City').Flow.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_sim = flows_native.append([flows_merged, flows_pruned_merged])\n",
    "\n",
    "df = df_all_sim.groupby('City').get_group('Southport')\n",
    "fig, ax = plt.subplots()\n",
    "for name, group in df.groupby('Simulation'):\n",
    "    group.plot(x=\"Timestep\", y='Flow', ax=ax, label=name)\n",
    "\n",
    "plt.ylabel(\"Flow (cms)\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.title(\"Southport - near basin outlet\")\n",
    "plt.xlim([1000, 1700])\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/southport_rising_limb.png\", bbox_inches='tight')\n",
    "\n",
    "df = df_all_sim.groupby('City').get_group('Wilmington')\n",
    "fig, ax = plt.subplots()\n",
    "for name, group in df.groupby('Simulation'):\n",
    "    group.plot(x=\"Timestep\", y='Flow', ax=ax, label=name)\n",
    "\n",
    "plt.ylabel(\"Flow (cms)\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.title(\"Wilmington - shortest network length\")\n",
    "plt.xlim([1000, 1700])\n",
    "plt.show()\n",
    "fig.savefig(\"../doc/wilmington_rising_limb.png\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the merging process change the parameter distributions?\n",
    "\n",
    "# merge all parameter data frames together with new indicator variable\n",
    "data['sim'] = \"native\"\n",
    "data_merged['sim'] = \"merged\"\n",
    "data_pruned_merged['sim'] = \"pruned/merged\"\n",
    "data_all = data.append([data_merged, data_pruned_merged])\n",
    "\n",
    "# add stream order\n",
    "data_all['order'] = \"\"\n",
    "for i in gdf.index:\n",
    "    data_all.loc[i, 'order'] = gdf.loc[i].order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['log_So'] = np.log(data_all.So)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=\"order\", y=\"log_So\", hue=\"sim\", data=data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.groupby('sim').So.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
