{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import glob\n",
    "import pathlib\n",
    "\n",
    "# logic to accomodate Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    ENV_IS_CL = True\n",
    "\n",
    "    root = pathlib.Path(\"/content/t-route\").resolve()\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"-b\",\n",
    "            \"segment-merge\",\n",
    "            \"https://github.com/awlostowski-noaa/t-route.git\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ! pip install geopandas\n",
    "    ! pip install zipfile\n",
    "    ! pip install us\n",
    "    ! pip install netcdf4\n",
    "\n",
    "except:\n",
    "    ENV_IS_CL = False\n",
    "    root = pathlib.Path(\"..\").resolve()\n",
    "\n",
    "sys.path.append(os.path.join(root, \"src\", \"python_framework_v02\"))\n",
    "sys.path.append(os.path.join(root, \"src\", \"python_framework_v01\"))\n",
    "\n",
    "# load needed scientific libraries\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter, deque\n",
    "from itertools import chain\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import us\n",
    "import time\n",
    "\n",
    "# turn off warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# import t-route functions for network analysis\n",
    "import nhd_io\n",
    "import nhd_network\n",
    "import network_dl\n",
    "import nhd_network_utilities_v02 as nnu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Load and mask network data - Cape Fear River Basin, North Carolina\n",
    "#####################################################################################\n",
    "# Create directory path variable for test/input/geo, where NHD data and masks are stored\n",
    "test_folder = os.path.join(root, r\"test\")\n",
    "geo_input_folder = os.path.join(test_folder, r\"input\", r\"geo\")\n",
    "\n",
    "# Load network meta data for the Cape Fear Basin\n",
    "supernetwork = \"CapeFear_FULL_RES\"\n",
    "network_data = nnu.set_supernetwork_data(\n",
    "    supernetwork=supernetwork, geo_input_folder=geo_input_folder\n",
    ")\n",
    "\n",
    "# if the NHDPlus RouteLink file does not exist, download it.\n",
    "if not os.path.exists(network_data[\"geo_file_path\"]):\n",
    "    filename = os.path.basename(network_data[\"geo_file_path\"])\n",
    "    network_dl.download(network_data[\"geo_file_path\"], network_data[\"data_link\"])\n",
    "\n",
    "# read-in NHD data, retain a copy for viz purposes\n",
    "data = nhd_io.read(network_data[\"geo_file_path\"])\n",
    "dat_geo = data.copy()\n",
    "\n",
    "# select only the necessary columns needed for network analysis and computation\n",
    "cols = [v for c, v in network_data.items() if c.endswith(\"_col\")]\n",
    "data = data[cols]\n",
    "\n",
    "# set the Data Frame index (row labels) to the key column - \"featureID\"\n",
    "data = data.set_index(network_data[\"key_col\"])\n",
    "dat_geo = dat_geo.set_index(network_data[\"key_col\"])\n",
    "\n",
    "# mask NHDNetwork to isolate test network - full resolution Cape Fear basin, NC\n",
    "if \"mask_file_path\" in network_data:\n",
    "    data_mask = nhd_io.read_mask(\n",
    "        network_data[\"mask_file_path\"], layer_string=network_data[\"mask_layer_string\"],\n",
    "    )\n",
    "    data = data.filter(data_mask.iloc[:, network_data[\"mask_key\"]], axis=0)\n",
    "    dat_geo = dat_geo.filter(data_mask.iloc[:, network_data[\"mask_key\"]], axis=0)\n",
    "\n",
    "# sort data by index\n",
    "data = data.sort_index()\n",
    "\n",
    "print(\"NHD data loaded and masked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Visualize the Cape Fear basin\n",
    "#####################################################################################\n",
    "\n",
    "# create a pandas GeoDataFrame for viz purposes\n",
    "gdf = gpd.GeoDataFrame(dat_geo, geometry=gpd.points_from_xy(dat_geo.lon, dat_geo.lat))\n",
    "\n",
    "################################################\n",
    "# Download North Carolina shapefile for basemap\n",
    "################################################\n",
    "shape_url = us.states.NC.shapefile_urls()[\"state\"]\n",
    "local_path = \"../test/input/geo/NorthCarolina_state_shape\"\n",
    "\n",
    "# Download and extract shapefile\n",
    "r = requests.get(shape_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(path=local_path)\n",
    "\n",
    "# get extracted filenames\n",
    "filenames = [\n",
    "    y\n",
    "    for y in sorted(z.namelist())\n",
    "    for ending in [\"dbf\", \"prj\", \"shp\", \"shx\"]\n",
    "    if y.endswith(ending)\n",
    "]\n",
    "dbf, prj, shp, shx = [filename for filename in filenames]\n",
    "\n",
    "# load shapefile with GeoPandas\n",
    "nc = gpd.read_file(local_path + \"/\" + shp)\n",
    "\n",
    "################################################\n",
    "# Create a map\n",
    "################################################\n",
    "\n",
    "# create a basemap\n",
    "base = nc.plot(color=\"white\", edgecolor=\"black\")\n",
    "\n",
    "# add our stream network, segments are represented as small points (nodes in the network)\n",
    "gdf.plot(ax=base, markersize=0.01, color=\"black\")\n",
    "\n",
    "# crop the map extent\n",
    "base.set_xlim(-85, -75)\n",
    "base.set_ylim(33, 37)\n",
    "\n",
    "# turn axes off\n",
    "base.axis(\"off\")\n",
    "\n",
    "# get current figure and set size\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 15)\n",
    "\n",
    "# add a title and display\n",
    "plt.title(\"Cape Fear River Basin - Full Resolution NHD\", fontsize=20)\n",
    "\n",
    "# display the map in output\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Analyze the distribution of segment lengths in the native NHD Full Res data\n",
    "#####################################################################################\n",
    "\n",
    "################################################\n",
    "# Create a histogram of segment lengths\n",
    "################################################\n",
    "\n",
    "data.Length.hist(bins=100)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.set_xlabel(\"Segment Length (m)\", size=14)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "################################################\n",
    "# What fraction of segments on the network are\n",
    "# shorter than specified threshold?\n",
    "################################################\n",
    "\n",
    "# specify threshold length\n",
    "thresh = 1000  # meters\n",
    "\n",
    "short_segs = len(data[data.Length <= thresh].index)\n",
    "total_segs = len(data.index)\n",
    "\n",
    "print(\n",
    "    \"There are %s segments in the supernetwork shorter than the threshold length.\"\n",
    "    % (short_segs)\n",
    ")\n",
    "print(\n",
    "    \"That is %s%% of all segments in the supernetwork!\"\n",
    "    % round(short_segs / total_segs * 100)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Extract network connections\n",
    "#####################################################################################\n",
    "# replace downstreams for terminal segments\n",
    "# for more details on this step, see network-analysis.ipynb\n",
    "data = nhd_io.replace_downstreams(data, network_data[\"downstream_col\"], 0)\n",
    "\n",
    "# extract the downstream connections of each segment\n",
    "connections = nhd_network.extract_connections(data, network_data[\"downstream_col\"])\n",
    "\n",
    "# reverse the network - extract the upstream connections of each segment\n",
    "rconn = nhd_network.reverse_network(connections)\n",
    "\n",
    "print(\"Network connections extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headwater_connections(connections, rconn):\n",
    "\n",
    "    \"\"\"\n",
    "    Determine which segments are and are not headwaters. \n",
    "    Headwaters are defined as segments with no upstream connection, only downstream connections. \n",
    "    Non-headwaters are defined as segments with both upstream and downstream connections. \n",
    "\n",
    "    Args:\n",
    "        connections (dict): downstream connections\n",
    "        rconn (dict): upstream connections\n",
    "\n",
    "    Returns:\n",
    "        hw_connections (dict): downstream connections, headwaters only\n",
    "        non_hw_connections (dict): downstream connections, non headwaters only\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    hw = []  # store headwater segments\n",
    "    non_hw = []  # store non-headwater segments\n",
    "\n",
    "    for seg in rconn.keys():\n",
    "        # if there is no upstream connection anda downstream connection, it is a headwater\n",
    "        if bool(rconn[seg]) == False and bool(connections[seg]) == True:\n",
    "            hw.append(seg)\n",
    "\n",
    "        # if there is an upstream connection and a downstream connection, it is a non-headwater (midwater?)\n",
    "        elif bool(rconn[seg]) == True and bool(connections[seg]) == True:\n",
    "            non_hw.append(seg)\n",
    "\n",
    "    # get segment key-value pairs from the connections dictionary\n",
    "    hw_connections = {key: connections[key] for key in hw}\n",
    "    non_hw_connections = {key: connections[key] for key in non_hw}\n",
    "\n",
    "    return hw_connections, non_hw_connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_headwaters(chop, hw_connections, non_hw_connections, data, thresh):\n",
    "\n",
    "    \"\"\"\n",
    "    Prune headwaters from the network\n",
    "    Headwaters are pruned if they are shorter than the threshold length\n",
    "    OR if they merge with a midwater that is less than the threshold length\n",
    "\n",
    "    Args:\n",
    "        chop (list): A list of pruned headwater indices\n",
    "        hw_connections (dict): downstream connections, headwaters only\n",
    "        non_hw_connections (dict): downstream connections, midwaters only\n",
    "        data (DataFrame): Network to be pruned\n",
    "        thresh (int): length threshold, segments below this length are slated for pruning\n",
    "\n",
    "    Returns:\n",
    "        chop (list): list of pruned headwater indices, updated\n",
    "        data_pruned (DataFrame): pruned network\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # loop through keys and values in headwater connections dictionary\n",
    "    for hw_k, srch_v in hw_connections.items():\n",
    "\n",
    "        \"\"\"\n",
    "        Trim headwater-headwater junctions\n",
    "        \n",
    "        - Headwater-headwater junctions are junctions where two (or more) headwaters merge. \n",
    "        - Once headwater-headwater junctions are identified, the shortest headwater below the\n",
    "          threshold length is pruned. \n",
    "        \"\"\"\n",
    "\n",
    "        # create a list of headwaters draining to the specified downstream connections, srch_v\n",
    "        G = [k for k, v in hw_connections.items() if v == srch_v]\n",
    "\n",
    "        # if more than one headwater drains into the segment, it is a headwater-headwater junction\n",
    "        if len(G) > 1:\n",
    "\n",
    "            # get segment lengths from the test dataset\n",
    "            hw_len = data.loc[G].Length\n",
    "\n",
    "            # find the shortest segment\n",
    "            hw_min_len = hw_len[hw_len.idxmin()]\n",
    "\n",
    "            # check to see if headwater shorter than threshold length\n",
    "            if hw_min_len < thresh:\n",
    "\n",
    "                # update chopping block\n",
    "                chop.append(hw_len.idxmin())\n",
    "\n",
    "        ############################################################\n",
    "        #  Trim headwater-midwater junctions\n",
    "        ############################################################\n",
    "\n",
    "        \"\"\"\n",
    "        Trim headwater-midwater junctions\n",
    "        \n",
    "        - Headwater-midwater junctions are junctions a headwater merges with a non-headwater (i.e. midwater). \n",
    "        - If the headwater is shorter than the threshold, it is pruned.\n",
    "        - If the headwater is longer than the threshold, but joins with or connects to a midwater that is\n",
    "          shorter than the threshold, the headwater is pruned. \n",
    "              - This is done to minimize the number of small midwater segments stranded between headwaters.\n",
    "                Else, these short, stranded segments cannot be merged upstream or downstream. \n",
    "        \"\"\"\n",
    "\n",
    "        # midwaters draining into a junction that a headwater does too\n",
    "        H = [k for k, v in non_hw_connections.items() if v == srch_v]\n",
    "\n",
    "        # if there is a corresponding midwater, then the headwater is a candidate for trimming\n",
    "        if bool(H) == True:\n",
    "\n",
    "            # get segment lengths from the dataset\n",
    "            hw_len = data.loc[hw_k].Length\n",
    "\n",
    "            # get midwater (in) length from the test dataset\n",
    "            mw_in_len = data.loc[H[0]].Length\n",
    "\n",
    "            # get midwater (out) length from the test dataset\n",
    "            mw_out_len = data.loc[srch_v[0]].Length\n",
    "\n",
    "            # check to see if headwater or midwaters (in/out) are shorter than the threshold\n",
    "            if hw_len < thresh or mw_in_len < thresh or mw_out_len < thresh:\n",
    "\n",
    "                # update chopping block\n",
    "                chop.append(hw_k)\n",
    "\n",
    "    # chop segments from the network\n",
    "    data_pruned = data.drop(chop)\n",
    "\n",
    "    return chop, data_pruned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Prune headwaters\n",
    "#####################################################################################\n",
    "\n",
    "# identify headwater and non-headwater (midwater) reaches (that also have a downstream connection)\n",
    "hw_connections, non_hw_connections = headwater_connections(connections, rconn)\n",
    "\n",
    "# Prune the network\n",
    "print(\"Pruning headwaters...\")\n",
    "\n",
    "pruned_headwaters = []\n",
    "pruned_headwaters, data_pruned = prune_headwaters(\n",
    "    pruned_headwaters, hw_connections, non_hw_connections, data, thresh\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Done pruning! Removed\",\n",
    "    len(pruned_headwaters),\n",
    "    \"headwater segments, or\",\n",
    "    int(data.loc[pruned_headwaters].Length.sum() / 1000),\n",
    "    \" river km, from the network.\",\n",
    ")\n",
    "print(\n",
    "    \"That is\",\n",
    "    int((data.loc[pruned_headwaters].Length.sum() / data.Length.sum()) * 100),\n",
    "    \"% of the total network length!\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Recompute connections and construct reaches from the pruned network data\n",
    "#####################################################################################\n",
    "\n",
    "# extract downstream connections for each node\n",
    "connections_pruned = nhd_network.extract_connections(\n",
    "    data_pruned, network_data[\"downstream_col\"]\n",
    ")\n",
    "\n",
    "# reverse the network - track upstream connections\n",
    "rconn_pruned = nhd_network.reverse_network(connections_pruned)\n",
    "\n",
    "# isolate independent subnetworks\n",
    "subnets = nhd_network.reachable_network(rconn_pruned)\n",
    "\n",
    "# identify the segments in each subnetwork\n",
    "subreachable = nhd_network.reachable(rconn_pruned)\n",
    "\n",
    "# break each subnetwork into reaches\n",
    "subreaches = {}\n",
    "for tw, net in subnets.items():\n",
    "    path_func = partial(nhd_network.split_at_junction, net)\n",
    "    subreaches[tw] = nhd_network.dfs_decomposition(net, path_func)\n",
    "\n",
    "print(\"Network connections extracted and reaches created from pruned network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Count segments that can be merged in the pruned network\n",
    "#####################################################################################\n",
    "counter_segments = 0\n",
    "counter_segments_orphaned = 0\n",
    "counter_short_singseg_reaches = 0\n",
    "counter_short_multiseg_reaches = 0\n",
    "counter_long_multiseg_partialmerger_reaches = 0\n",
    "\n",
    "for twi, (tw, rchs) in enumerate(subreaches.items(), 1):\n",
    "\n",
    "    for rch in rchs:\n",
    "\n",
    "        rch_len = data_pruned.loc[rch].Length.sum()\n",
    "\n",
    "        ##################################################\n",
    "        # orphaned short single segment reaches\n",
    "        ##################################################\n",
    "        if rch_len < thresh and len(data_pruned.loc[rch]) == 1:\n",
    "\n",
    "            counter_short_singseg_reaches += 1\n",
    "            counter_segments_orphaned += 1\n",
    "\n",
    "        ##################################################\n",
    "        # multi segment reaches - combine into a single segment reach\n",
    "        ##################################################\n",
    "        if rch_len < thresh and len(data_pruned.loc[rch]) > 1:\n",
    "\n",
    "            counter_short_multiseg_reaches += 1\n",
    "            counter_segments += len(rch)\n",
    "\n",
    "        ##################################################\n",
    "        # multi segment reaches longer than threshold with some segments shorter than threshold\n",
    "        ##################################################\n",
    "        if rch_len > thresh and data_pruned.loc[rch].Length.min() < thresh:\n",
    "\n",
    "            counter_long_multiseg_partialmerger_reaches += 1\n",
    "            counter_segments += len(data_pruned.loc[rch].Length < thresh)\n",
    "\n",
    "print(\n",
    "    \"There are\",\n",
    "    counter_short_singseg_reaches,\n",
    "    \"short single segment reaches that cannot be merged.\",\n",
    ")\n",
    "print(\n",
    "    \"There are\",\n",
    "    counter_short_multiseg_reaches,\n",
    "    \"short multi-segment reaches that can be completely merged.\",\n",
    ")\n",
    "print(\n",
    "    \"There are\",\n",
    "    counter_long_multiseg_partialmerger_reaches,\n",
    "    \"long multi-segment reaches that can be partially merged.\",\n",
    ")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"We can successfully merge-out\", counter_segments, \"river segments.\")\n",
    "print(counter_segments_orphaned, \"short segments will remain that cannot be merged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_weighted_av(df, var, weight):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate a weighted average\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing variables to be averaged and used as weights\n",
    "        var (str): name of the variable to be averaged\n",
    "        weight (str): name of the variable to be used as a weight\n",
    "\n",
    "    Returns:\n",
    "        x (float32): weighted average\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x = (df[weight] * df[var]).sum() / df[weight].sum()\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parameters(to_merge):\n",
    "\n",
    "    \"\"\"\n",
    "    length-weighted averaging of channel routing parameters across merged segments\n",
    "\n",
    "    Args:\n",
    "        to_merge (DataFrame): DataFrame containing routing parameters for segments to be merged together\n",
    "\n",
    "    Returns:\n",
    "        replace (DataFrame): weighted average\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data_replace = to_merge.tail(1)\n",
    "\n",
    "    idx = to_merge.tail(1).index\n",
    "    data_replace.loc[idx, \"Length\"] = to_merge.Length.sum()\n",
    "    data_replace.loc[idx, \"n\"] = len_weighted_av(to_merge, \"n\", \"Length\")\n",
    "    data_replace.loc[idx, \"nCC\"] = len_weighted_av(to_merge, \"nCC\", \"Length\")\n",
    "    data_replace.loc[idx, \"So\"] = len_weighted_av(to_merge, \"So\", \"Length\")\n",
    "    data_replace.loc[idx, \"BtmWdth\"] = len_weighted_av(to_merge, \"BtmWdth\", \"Length\")\n",
    "    data_replace.loc[idx, \"TopWdth\"] = len_weighted_av(to_merge, \"TopWdth\", \"Length\")\n",
    "    data_replace.loc[idx, \"TopWdthCC\"] = len_weighted_av(\n",
    "        to_merge, \"TopWdthCC\", \"Length\"\n",
    "    )\n",
    "    data_replace.loc[idx, \"MusK\"] = len_weighted_av(to_merge, \"MusK\", \"Length\")\n",
    "    data_replace.loc[idx, \"MusX\"] = len_weighted_av(to_merge, \"MusX\", \"Length\")\n",
    "    data_replace.loc[idx, \"ChSlp\"] = len_weighted_av(to_merge, \"ChSlp\", \"Length\")\n",
    "\n",
    "    return data_replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_reach_connections(data_merged):\n",
    "\n",
    "    \"\"\"\n",
    "    Update downstream connections (\"to\") for segments in a merged reach.\n",
    "    Only updates *in-reach* connections.\n",
    "\n",
    "    Args:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach\n",
    "\n",
    "    Returns:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach with updated donwstream connections\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for i, idx in enumerate(data_merged.index.values[0:-1]):\n",
    "        data_merged.loc[idx, \"to\"] = data_merged.index.values[i + 1]\n",
    "\n",
    "    return data_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upstream_merge(data_merged, chop):\n",
    "\n",
    "    \"\"\"\n",
    "    Merge a short reach tail segment with upstream neighbor\n",
    "\n",
    "    Args:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach\n",
    "        chop (list): list of merged-out segments\n",
    "\n",
    "    Returns:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach with updated donwstream connections\n",
    "        chop (list): updated list of merged-out segments\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # grab the two segments that need to be merged - simply the last two segments of the reach\n",
    "    to_merge = data_merged.tail(2)\n",
    "\n",
    "    # calculate new parameter values\n",
    "    data_replace = merge_parameters(to_merge)\n",
    "\n",
    "    # paste new parameters in to data_merged\n",
    "    data_merged.loc[to_merge.tail(1).index] = data_replace\n",
    "\n",
    "    # remove merged segments from data_merged\n",
    "    data_merged = data_merged.drop(to_merge.head(1).index)\n",
    "\n",
    "    # update \"chop\" list with merged-out segment IDs\n",
    "    chop.append(to_merge.head(1).index.values[0])\n",
    "\n",
    "    return data_merged, chop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downstream_merge(data_merged, chop, thresh):\n",
    "\n",
    "    \"\"\"\n",
    "    Merge short segments with their downstream neighbors\n",
    "\n",
    "    Args:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach\n",
    "        chop (list): list of merged-out segments\n",
    "        thresh (int): theshold reach length (meters)\n",
    "\n",
    "    Returns:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach with updated donwstream connections\n",
    "        chop (list): updated list of merged-out segments\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # find the upstream-most short segment and it's downstream connection\n",
    "    idx_us = data_merged.loc[data_merged.Length < thresh].head(1).index.values[0]\n",
    "\n",
    "    pos_idx_us = data_merged.index.get_loc(idx_us)\n",
    "    idx_to = data_merged.iloc[pos_idx_us + 1].name\n",
    "\n",
    "    # grab segments to be merged\n",
    "    to_merge = data_merged.loc[[idx_us, idx_to]]\n",
    "\n",
    "    # calculate new parameter values\n",
    "    data_replace = merge_parameters(to_merge)\n",
    "\n",
    "    # paste new parameters in to data_merged\n",
    "    data_merged.loc[to_merge.tail(1).index] = data_replace\n",
    "\n",
    "    # remove merged segments from data_merged\n",
    "    data_merged = data_merged.drop(to_merge.head(1).index)\n",
    "\n",
    "    # update \"chop\" list with merged-out segment IDs\n",
    "    chop.append(to_merge.head(1).index.values[0])\n",
    "\n",
    "    return data_merged, chop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all(rch, data, chop):\n",
    "\n",
    "    \"\"\"\n",
    "    Merge all segments in a reach\n",
    "\n",
    "    Args:\n",
    "        rch (list): Segment indices in the reach to be merged\n",
    "        data (DataFrame): Routing parameters for network containing the reach to be merged\n",
    "        chop (list): list of merged-out segments\n",
    "\n",
    "    Returns:\n",
    "        data_merged (DataFrame): Routing parameters for segments in merged reach with updated donwstream connections\n",
    "        chop (list): updated list of merged-out segments\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # subset the model parameter data for this reach\n",
    "    data_merged = data.loc[rch].copy()\n",
    "\n",
    "    # grab the two segments that need to be merged - in this case, merge all segments!\n",
    "    to_merge = data_merged.copy()\n",
    "\n",
    "    # calculate new parameter values\n",
    "    data_replace = merge_parameters(to_merge)\n",
    "\n",
    "    # paste new parameters in to data_merged\n",
    "    data_merged.loc[to_merge.tail(1).index] = data_replace\n",
    "\n",
    "    # remove merged segments from data_merged - in this case, all but the last\n",
    "    data_merged = data_merged.drop(data_merged.iloc[:-1, :].index)\n",
    "\n",
    "    # update \"chop\" list with merged-out segment IDs\n",
    "    chop.extend(list(to_merge.iloc[:-1, :].index))\n",
    "\n",
    "    return data_merged, chop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_length(data, data_merged):\n",
    "\n",
    "    \"\"\"\n",
    "    Check the merged network to the total stream length has not changed.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Routing parameters for pre-merged network\n",
    "        data_merged (DataFrame): Routing parameters for merged reach\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if data.Length.sum() != data_merged.Length.sum():\n",
    "        print(\"Error: The merging process has changed the network length\")\n",
    "    else:\n",
    "        print(\"Good stuff: The merging process did not change the network length!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network_data(data, rch, data_merged, chop, rconn):\n",
    "\n",
    "    \"\"\"\n",
    "    Update the network routing parameter data with merged segment data\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Routing parameters for network to be updated\n",
    "        rch (list): Segment indices in the reach to be merged\n",
    "        data_merged (DataFrame): Routing parameters for merged reach\n",
    "\n",
    "    Returns:\n",
    "        data (DataFrame): Updated network routing parameters\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # make a copy of the data to be replaced with merged\n",
    "    data_old = data.loc[rch].copy()\n",
    "\n",
    "    # drop the segments that disapeared with merger\n",
    "    data = data.drop(chop)\n",
    "\n",
    "    # adjust the segment data for those that remain\n",
    "    data.loc[data_merged.index] = data_merged\n",
    "\n",
    "    # update out of reach connections - these will change in the first segment was merged out\n",
    "    upstreams = rconn[\n",
    "        data_old.head(1).index.values[0]\n",
    "    ]  # upstream connection of the OLD reach head\n",
    "\n",
    "    if bool(upstreams):\n",
    "        data.loc[upstreams, \"to\"] = data_merged.head(1).index.values[\n",
    "            0\n",
    "        ]  # index of NEW reach head\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_network(data, data_merged, network_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Check the merged network to ensure subnetwork structure has not changed.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Routing parameters for pre-merged network\n",
    "        data_merged (DataFrame): Routing parameters for merged network\n",
    "        network_data (dict): supernetwork meta data containing the column name of the downstream connection column\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # extract downstream connections for each node\n",
    "    connections_merged = nhd_network.extract_connections(\n",
    "        data_merged, network_data[\"downstream_col\"]\n",
    "    )\n",
    "    connections = nhd_network.extract_connections(data, network_data[\"downstream_col\"])\n",
    "\n",
    "    # reverse the network - track upstream connections\n",
    "    rconn_merged = nhd_network.reverse_network(connections_merged)\n",
    "    rconn = nhd_network.reverse_network(connections)\n",
    "\n",
    "    # isolate independent subnetworks\n",
    "    subnets_merged = nhd_network.reachable_network(rconn_merged)\n",
    "    subnets = nhd_network.reachable_network(rconn)\n",
    "\n",
    "    # compare the number of tailwaters\n",
    "    if len(subnets_merged.keys()) != len(subnets.keys()):\n",
    "        print(\"Error: merging process has changed number of subnetworks!\")\n",
    "    else:\n",
    "        print(\"Good stuff! The merging process has preserved the number of subnetworks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Merge-out short segments\n",
    "#####################################################################################\n",
    "\n",
    "# create a copy of the pruned network dataset, which will be updated with merged data\n",
    "data_pruned_merged = data_pruned.copy()\n",
    "\n",
    "# initialize list to store merged segment IDs\n",
    "merged_segments = []\n",
    "\n",
    "start = time.time()\n",
    "print(\"Merging-out short segments, this will take ~2 minutes\")\n",
    "# loop through each reach in the network\n",
    "for twi, (tw, rchs) in enumerate(subreaches.items(), 1):\n",
    "\n",
    "    for rch in rchs:\n",
    "\n",
    "        # calculate reach length\n",
    "        rch_len = data_pruned.loc[rch].Length.sum()\n",
    "\n",
    "        ##################################################\n",
    "        # orphaned short single segment reaches\n",
    "        ##################################################\n",
    "        # if reach length is shorter than threshold and composed of a single segment\n",
    "        if rch_len < thresh and len(data_pruned.loc[rch]) == 1:\n",
    "\n",
    "            continue  # do nothing\n",
    "\n",
    "        ##################################################\n",
    "        # multi segment reaches - combine into a single segment reach\n",
    "        ##################################################\n",
    "        # if reach length is shorter than threshold and composed more than one segment\n",
    "        if rch_len < thresh and len(data_pruned.loc[rch]) > 1:\n",
    "\n",
    "            # merge ALL reach segments into one\n",
    "            chop = []\n",
    "            data_merged, chop = merge_all(rch, data_pruned, chop)\n",
    "\n",
    "            # update network with merged data\n",
    "            data_pruned_merged = update_network_data(\n",
    "                data_pruned_merged, rch, data_merged, chop, rconn_pruned\n",
    "            )\n",
    "\n",
    "            # update merged_segmetns list with merged-out segments\n",
    "            merged_segments.extend(chop)\n",
    "\n",
    "        ##################################################\n",
    "        # multi segment reaches longer than threshold with some segments shorter than threshold\n",
    "        ##################################################\n",
    "        # if reach length is longer than threshold and smallest segment length is less than threshold\n",
    "        if rch_len > thresh and data_pruned.loc[rch].Length.min() < thresh:\n",
    "\n",
    "            # initialize data_merged - this DataFrame will be subsequently revised\n",
    "            data_merged = data_pruned.loc[rch]\n",
    "\n",
    "            # initialize list of segments chopped from this reach\n",
    "            chop_reach = []\n",
    "\n",
    "            # so long as the shortest segment is shorter than the threshold...\n",
    "            while data_merged.Length.min() < thresh:\n",
    "\n",
    "                # if shortest segment is the last segment in the reach - conduct an upstream merge.\n",
    "                if (\n",
    "                    data_merged.Length.idxmin() == data_merged.tail(1).index.values[0]\n",
    "                    and data_merged.Length.min() < thresh\n",
    "                ):\n",
    "\n",
    "                    # upstream merge\n",
    "                    chop = []\n",
    "                    data_merged, chop = upstream_merge(data_merged, chop)\n",
    "\n",
    "                    # update chop_reach list with merged-out segments\n",
    "                    chop_reach.extend(chop)\n",
    "\n",
    "                # if shortest segment is NOT the last segment in the reach - conduct a downstream merge\n",
    "                if (\n",
    "                    data_merged.Length.idxmin() != data_merged.tail(1).index.values[0]\n",
    "                    and data_merged.Length.min() < thresh\n",
    "                ):\n",
    "\n",
    "                    # downstream merge\n",
    "                    chop = []\n",
    "                    data_merged, chop = downstream_merge(data_merged, chop, thresh)\n",
    "\n",
    "                    # update chop_reach list with merged-out segments\n",
    "                    chop_reach.extend(chop)\n",
    "\n",
    "            # correct segment connections within reach\n",
    "            data_merged = correct_reach_connections(data_merged)\n",
    "\n",
    "            # update the greater network data set\n",
    "            data_pruned_merged = update_network_data(\n",
    "                data_pruned_merged, rch, data_merged, chop_reach, rconn_pruned\n",
    "            )\n",
    "\n",
    "end = time.time()\n",
    "# check that the merging process does not change the length of the network\n",
    "check_length(data_pruned, data_pruned_merged)\n",
    "\n",
    "# check that the merging process does not change the number of tailwaters in the supernetwork\n",
    "check_network(data_pruned, data_pruned_merged, network_data)\n",
    "\n",
    "print(\"The merging process took\", round((end - start) / 60), \"minutes to complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# How did pruning and merging change the distribution of segment lengths?\n",
    "#####################################################################################\n",
    "\n",
    "##################################################\n",
    "data.Length.hist(bins=100)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.set_xlabel(\"Segment Length (m)\", size=14)\n",
    "plt.title(\"native network\", fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##################################################\n",
    "data_pruned.Length.hist(bins=100)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.set_xlabel(\"Segment Length (m)\", size=14)\n",
    "plt.title(\"pruned network\", fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##################################################\n",
    "data_pruned_merged.Length.hist(bins=100)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.set_xlabel(\"Segment Length (m)\", size=14)\n",
    "plt.title(\"pruned and merged network\", fontsize=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
